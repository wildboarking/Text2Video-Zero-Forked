{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1961b60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637175f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\jby19\\MyProjects\\Text2Videozero\\annotator\\openpose\\body.py:5: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n",
      "C:\\Users\\jby19\\MyProjects\\Text2Videozero\\annotator\\openpose\\hand.py:6: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\skimage\\util\\dtype.py:27: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Model\n",
    "\n",
    "model = Model(device = \"cuda\", dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a6d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ba8b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module Text2Video\n",
      "Model update\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ch_model.safetensors: 100%|██████████| 1.72G/1.72G [04:51<00:00, 5.90MB/s]\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jby19\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)ain/unet/config.json: 100%|██████████| 901/901 [00:00<00:00, 301kB/s]\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\safetensors\\torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "Downloading (…)ain/model_index.json: 100%|██████████| 511/511 [00:00<00:00, 179kB/s]\n",
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "Downloading (…)cheduler_config.json: 100%|██████████| 341/341 [00:00<00:00, 68.4kB/s]\n",
      "\n",
      "Fetching 12 files:  17%|█▋        | 2/12 [00:00<00:01,  6.33it/s][00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading (…)_encoder/config.json: 100%|██████████| 617/617 [00:00<00:00, 124kB/s]\n",
      "\n",
      "\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 807/807 [00:00<00:00, 162kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 472/472 [00:00<00:00, 78.9kB/s][A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)a0d5/vae/config.json: 100%|██████████| 577/577 [00:00<00:00, 144kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)tokenizer/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 734kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)tokenizer/vocab.json: 100%|██████████| 1.06M/1.06M [00:01<00:00, 774kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:   1%|          | 10.5M/1.72G [00:03<09:36, 2.97MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   4%|▍         | 10.5M/246M [00:05<01:58, 1.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:   6%|▋         | 10.5M/167M [00:06<01:37, 1.60MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   1%|          | 21.0M/1.72G [00:07<10:53, 2.60MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   9%|▊         | 21.0M/246M [00:09<01:37, 2.31MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   2%|▏         | 31.5M/1.72G [00:11<10:17, 2.73MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  13%|█▎        | 21.0M/167M [00:13<01:35, 1.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 31.5M/246M [00:14<01:35, 2.24MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   2%|▏         | 41.9M/1.72G [00:15<10:53, 2.57MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  17%|█▋        | 41.9M/246M [00:19<01:33, 2.19MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  19%|█▉        | 31.5M/167M [00:20<01:26, 1.57MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   3%|▎         | 52.4M/1.72G [00:20<11:22, 2.44MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  21%|██▏       | 52.4M/246M [00:24<01:30, 2.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  25%|██▌       | 41.9M/167M [00:26<01:17, 1.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   4%|▎         | 62.9M/1.72G [00:27<13:13, 2.09MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  26%|██▌       | 62.9M/246M [00:29<01:29, 2.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  31%|███▏      | 52.4M/167M [00:35<01:21, 1.40MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   4%|▍         | 73.4M/1.72G [00:36<16:41, 1.64MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  30%|██▉       | 73.4M/246M [00:38<01:44, 1.66MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   5%|▍         | 83.9M/1.72G [00:43<16:52, 1.62MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  38%|███▊      | 62.9M/167M [00:43<01:15, 1.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  34%|███▍      | 83.9M/246M [00:44<01:36, 1.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 94.4M/246M [00:49<01:22, 1.85MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   5%|▌         | 94.4M/1.72G [00:49<16:43, 1.62MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  44%|████▍     | 73.4M/167M [00:49<01:04, 1.46MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  43%|████▎     | 105M/246M [00:54<01:16, 1.84MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  50%|█████     | 83.9M/167M [00:55<00:52, 1.58MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   6%|▌         | 105M/1.72G [00:55<16:16, 1.65MB/s] \u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  47%|████▋     | 115M/246M [00:59<01:08, 1.92MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   7%|▋         | 115M/1.72G [01:00<15:11, 1.76MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  56%|█████▋    | 94.4M/167M [01:00<00:44, 1.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  51%|█████     | 126M/246M [01:04<01:01, 1.95MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   7%|▋         | 126M/1.72G [01:05<14:20, 1.85MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  63%|██████▎   | 105M/167M [01:05<00:35, 1.76MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  55%|█████▌    | 136M/246M [01:09<00:53, 2.06MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   8%|▊         | 136M/1.72G [01:11<14:38, 1.80MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  69%|██████▉   | 115M/167M [01:11<00:29, 1.79MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  60%|█████▉    | 147M/246M [01:14<00:48, 2.06MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   9%|▊         | 147M/1.72G [01:16<14:04, 1.86MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  75%|███████▌  | 126M/167M [01:17<00:23, 1.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  64%|██████▍   | 157M/246M [01:18<00:41, 2.15MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:   9%|▉         | 157M/1.72G [01:22<13:43, 1.90MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 168M/246M [01:23<00:37, 2.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  81%|████████▏ | 136M/167M [01:23<00:17, 1.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  10%|▉         | 168M/1.72G [01:27<13:21, 1.94MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  72%|███████▏  | 178M/246M [01:28<00:31, 2.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  88%|████████▊ | 147M/167M [01:29<00:11, 1.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  10%|█         | 178M/1.72G [01:32<13:10, 1.95MB/s]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  77%|███████▋  | 189M/246M [01:32<00:25, 2.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin:  94%|█████████▍| 157M/167M [01:35<00:05, 1.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  81%|████████  | 199M/246M [01:36<00:20, 2.34MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  11%|█         | 189M/1.72G [01:38<13:03, 1.95MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading (…)on_pytorch_model.bin: 100%|██████████| 167M/167M [01:41<00:00, 1.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading pytorch_model.bin:  85%|████████▌ | 210M/246M [01:42<00:16, 2.20MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  12%|█▏        | 199M/1.72G [01:43<12:44, 1.99MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  89%|████████▉ | 220M/246M [01:45<00:11, 2.36MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  12%|█▏        | 210M/1.72G [01:46<11:16, 2.23MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  94%|█████████▎| 231M/246M [01:49<00:06, 2.51MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  13%|█▎        | 220M/1.72G [01:50<10:34, 2.36MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 241M/246M [01:53<00:02, 2.50MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  13%|█▎        | 231M/1.72G [01:54<10:05, 2.46MB/s]\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|██████████| 246M/246M [01:55<00:00, 2.13MB/s]\u001b[A\u001b[A\n",
      "Fetching 12 files:  33%|███▎      | 4/12 [01:55<04:32, 34.07s/it]\n",
      "Downloading (…)on_pytorch_model.bin:  14%|█▍        | 241M/1.72G [01:57<09:12, 2.68MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  15%|█▍        | 252M/1.72G [01:59<07:57, 3.07MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  15%|█▌        | 262M/1.72G [02:01<06:44, 3.60MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  16%|█▌        | 273M/1.72G [02:02<05:48, 4.15MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  16%|█▋        | 283M/1.72G [02:04<05:09, 4.64MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  17%|█▋        | 294M/1.72G [02:06<04:48, 4.94MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  18%|█▊        | 304M/1.72G [02:07<04:28, 5.28MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  18%|█▊        | 315M/1.72G [02:10<04:41, 4.99MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  19%|█▉        | 325M/1.72G [02:12<04:57, 4.69MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  20%|█▉        | 336M/1.72G [02:15<05:11, 4.45MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  20%|██        | 346M/1.72G [02:18<05:25, 4.22MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  21%|██        | 357M/1.72G [02:20<05:02, 4.50MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  21%|██▏       | 367M/1.72G [02:21<04:32, 4.96MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  22%|██▏       | 377M/1.72G [02:23<04:12, 5.32MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  23%|██▎       | 388M/1.72G [02:25<03:56, 5.62MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  23%|██▎       | 398M/1.72G [02:26<03:49, 5.76MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  24%|██▍       | 409M/1.72G [02:28<03:46, 5.79MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  24%|██▍       | 419M/1.72G [02:30<03:36, 5.99MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  25%|██▌       | 430M/1.72G [02:31<03:31, 6.10MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  26%|██▌       | 440M/1.72G [02:33<03:36, 5.91MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  26%|██▌       | 451M/1.72G [02:35<03:43, 5.67MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  27%|██▋       | 461M/1.72G [02:37<03:40, 5.71MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  27%|██▋       | 472M/1.72G [02:39<03:35, 5.79MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  28%|██▊       | 482M/1.72G [02:40<03:25, 6.02MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  29%|██▊       | 493M/1.72G [02:42<03:30, 5.84MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  29%|██▉       | 503M/1.72G [02:44<03:36, 5.62MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  30%|██▉       | 514M/1.72G [02:46<03:34, 5.63MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  30%|███       | 524M/1.72G [02:48<03:22, 5.90MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  31%|███       | 535M/1.72G [02:50<03:39, 5.39MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  32%|███▏      | 545M/1.72G [02:53<04:19, 4.52MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  32%|███▏      | 556M/1.72G [02:55<04:05, 4.73MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  33%|███▎      | 566M/1.72G [02:57<03:42, 5.17MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  34%|███▎      | 577M/1.72G [02:59<03:39, 5.21MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  34%|███▍      | 587M/1.72G [03:01<03:32, 5.32MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  35%|███▍      | 598M/1.72G [03:02<03:21, 5.57MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  35%|███▌      | 608M/1.72G [03:04<03:16, 5.66MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  36%|███▌      | 619M/1.72G [03:06<03:11, 5.74MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  37%|███▋      | 629M/1.72G [03:08<03:13, 5.64MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  37%|███▋      | 640M/1.72G [03:10<03:06, 5.78MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  38%|███▊      | 650M/1.72G [03:11<03:02, 5.85MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  38%|███▊      | 661M/1.72G [03:13<02:56, 6.01MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  39%|███▉      | 671M/1.72G [03:16<03:16, 5.33MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  40%|███▉      | 682M/1.72G [03:17<03:12, 5.39MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  40%|████      | 692M/1.72G [03:19<02:58, 5.75MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  41%|████      | 703M/1.72G [03:21<02:55, 5.80MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  41%|████▏     | 713M/1.72G [03:22<02:47, 6.00MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  42%|████▏     | 724M/1.72G [03:25<03:00, 5.52MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  43%|████▎     | 734M/1.72G [03:27<03:07, 5.26MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  43%|████▎     | 744M/1.72G [03:28<02:55, 5.55MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  44%|████▍     | 755M/1.72G [03:30<02:45, 5.82MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  45%|████▍     | 765M/1.72G [03:32<02:41, 5.90MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  45%|████▌     | 776M/1.72G [03:34<02:40, 5.88MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  46%|████▌     | 786M/1.72G [03:35<02:33, 6.06MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  46%|████▋     | 797M/1.72G [03:37<02:31, 6.10MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  47%|████▋     | 807M/1.72G [03:38<02:27, 6.19MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  48%|████▊     | 818M/1.72G [03:40<02:22, 6.31MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  48%|████▊     | 828M/1.72G [03:42<02:32, 5.82MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  49%|████▉     | 839M/1.72G [03:44<02:33, 5.74MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  49%|████▉     | 849M/1.72G [03:47<02:52, 5.04MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  50%|█████     | 860M/1.72G [03:49<02:45, 5.21MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  51%|█████     | 870M/1.72G [03:50<02:34, 5.50MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  51%|█████     | 881M/1.72G [03:53<02:42, 5.16MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  52%|█████▏    | 891M/1.72G [03:55<02:43, 5.08MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  52%|█████▏    | 902M/1.72G [03:56<02:30, 5.42MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  53%|█████▎    | 912M/1.72G [03:58<02:23, 5.62MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  54%|█████▎    | 923M/1.72G [04:00<02:15, 5.88MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  54%|█████▍    | 933M/1.72G [04:01<02:14, 5.84MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  55%|█████▍    | 944M/1.72G [04:03<02:09, 5.97MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  55%|█████▌    | 954M/1.72G [04:06<02:23, 5.33MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  56%|█████▌    | 965M/1.72G [04:07<02:15, 5.59MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  57%|█████▋    | 975M/1.72G [04:09<02:07, 5.82MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  57%|█████▋    | 986M/1.72G [04:11<02:05, 5.85MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  58%|█████▊    | 996M/1.72G [04:12<02:02, 5.92MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  59%|█████▊    | 1.01G/1.72G [04:14<01:58, 6.02MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)on_pytorch_model.bin:  59%|█████▉    | 1.02G/1.72G [04:16<01:54, 6.11MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  60%|█████▉    | 1.03G/1.72G [04:17<01:52, 6.16MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  60%|██████    | 1.04G/1.72G [04:19<01:51, 6.09MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  61%|██████    | 1.05G/1.72G [04:21<01:49, 6.13MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  62%|██████▏   | 1.06G/1.72G [04:23<01:49, 6.03MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  62%|██████▏   | 1.07G/1.72G [04:26<02:12, 4.90MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  63%|██████▎   | 1.08G/1.72G [04:28<02:06, 5.04MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  63%|██████▎   | 1.09G/1.72G [04:29<01:57, 5.37MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  64%|██████▍   | 1.10G/1.72G [04:31<01:50, 5.59MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  65%|██████▍   | 1.11G/1.72G [04:33<01:46, 5.73MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  65%|██████▌   | 1.12G/1.72G [04:34<01:41, 5.89MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  66%|██████▌   | 1.13G/1.72G [04:36<01:38, 5.94MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  66%|██████▋   | 1.14G/1.72G [04:38<01:34, 6.12MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  67%|██████▋   | 1.15G/1.72G [04:39<01:32, 6.13MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  68%|██████▊   | 1.16G/1.72G [04:41<01:29, 6.21MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  68%|██████▊   | 1.17G/1.72G [04:43<01:26, 6.30MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  69%|██████▉   | 1.18G/1.72G [04:44<01:24, 6.31MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  70%|██████▉   | 1.20G/1.72G [04:46<01:21, 6.40MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  70%|███████   | 1.21G/1.72G [04:48<01:21, 6.34MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  71%|███████   | 1.22G/1.72G [04:49<01:18, 6.41MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  71%|███████▏  | 1.23G/1.72G [04:51<01:23, 5.92MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  72%|███████▏  | 1.24G/1.72G [04:54<01:31, 5.27MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  73%|███████▎  | 1.25G/1.72G [04:56<01:35, 4.92MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  73%|███████▎  | 1.26G/1.72G [04:58<01:29, 5.15MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  74%|███████▍  | 1.27G/1.72G [05:00<01:21, 5.53MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  74%|███████▍  | 1.28G/1.72G [05:02<01:20, 5.46MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  75%|███████▌  | 1.29G/1.72G [05:03<01:15, 5.68MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  76%|███████▌  | 1.30G/1.72G [05:05<01:11, 5.87MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  76%|███████▌  | 1.31G/1.72G [05:07<01:08, 5.98MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  77%|███████▋  | 1.32G/1.72G [05:08<01:04, 6.14MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  77%|███████▋  | 1.33G/1.72G [05:10<01:07, 5.74MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  78%|███████▊  | 1.34G/1.72G [05:12<01:03, 5.93MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  79%|███████▊  | 1.35G/1.72G [05:13<00:59, 6.16MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  79%|███████▉  | 1.36G/1.72G [05:15<00:57, 6.24MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  80%|███████▉  | 1.37G/1.72G [05:17<00:54, 6.32MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  81%|████████  | 1.38G/1.72G [05:18<00:52, 6.40MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  81%|████████  | 1.39G/1.72G [05:20<00:51, 6.31MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  82%|████████▏ | 1.41G/1.72G [05:22<00:49, 6.41MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  82%|████████▏ | 1.42G/1.72G [05:23<00:48, 6.31MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  83%|████████▎ | 1.43G/1.72G [05:25<00:46, 6.24MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  84%|████████▎ | 1.44G/1.72G [05:27<00:44, 6.29MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  84%|████████▍ | 1.45G/1.72G [05:28<00:43, 6.25MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  85%|████████▍ | 1.46G/1.72G [05:30<00:43, 6.05MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  85%|████████▌ | 1.47G/1.72G [05:32<00:43, 5.84MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  86%|████████▌ | 1.48G/1.72G [05:34<00:39, 6.04MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  87%|████████▋ | 1.49G/1.72G [05:36<00:38, 6.05MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  87%|████████▋ | 1.50G/1.72G [05:37<00:35, 6.20MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  88%|████████▊ | 1.51G/1.72G [05:39<00:33, 6.27MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  88%|████████▊ | 1.52G/1.72G [05:41<00:35, 5.58MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  89%|████████▉ | 1.53G/1.72G [05:43<00:33, 5.57MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  90%|████████▉ | 1.54G/1.72G [05:45<00:30, 5.83MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  90%|█████████ | 1.55G/1.72G [05:46<00:28, 5.82MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  91%|█████████ | 1.56G/1.72G [05:48<00:26, 5.90MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  91%|█████████▏| 1.57G/1.72G [05:50<00:25, 5.81MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  92%|█████████▏| 1.58G/1.72G [05:52<00:22, 5.91MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  93%|█████████▎| 1.59G/1.72G [05:53<00:20, 6.02MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  93%|█████████▎| 1.60G/1.72G [05:55<00:19, 6.04MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  94%|█████████▍| 1.61G/1.72G [05:57<00:17, 6.12MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  95%|█████████▍| 1.63G/1.72G [05:58<00:15, 6.12MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  95%|█████████▌| 1.64G/1.72G [06:00<00:13, 6.16MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  96%|█████████▌| 1.65G/1.72G [06:02<00:11, 6.32MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  96%|█████████▋| 1.66G/1.72G [06:03<00:09, 6.35MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  97%|█████████▋| 1.67G/1.72G [06:05<00:08, 6.43MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  98%|█████████▊| 1.68G/1.72G [06:07<00:06, 6.30MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  98%|█████████▊| 1.69G/1.72G [06:08<00:05, 6.15MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  99%|█████████▉| 1.70G/1.72G [06:10<00:03, 6.28MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin:  99%|█████████▉| 1.71G/1.72G [06:12<00:01, 6.26MB/s]\u001b[A\n",
      "Downloading (…)on_pytorch_model.bin: 100%|██████████| 1.72G/1.72G [06:13<00:00, 4.60MB/s]\u001b[A\n",
      "Fetching 12 files: 100%|██████████| 12/12 [06:14<00:00, 31.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 / 2\n",
      "t0 = 881 t1 = 941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue DDIM with i = 0, t = 981, latent = torch.Size([1, 4, 64, 64]), device = cuda:0, type = torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:01<00:14,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent t1 found at i=1, t = 961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:01<00:07,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent t0 found at i = 4, t = 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  7.10it/s]\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\jby19\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue DDIM with i = 2, t = 941, latent = torch.Size([8, 4, 64, 64]), device = cuda:0, type = torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 8.00 GiB total capacity; 3.12 GiB already allocated; 1.71 GiB free; 4.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m44\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m47\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotion_field_strength_x\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m12\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotion_field_strength_y\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m12\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m}\n\u001b[0;32m      4\u001b[0m out_path, fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./text2video_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mprocess_text2video(prompt, fps \u001b[38;5;241m=\u001b[39m fps, path \u001b[38;5;241m=\u001b[39m out_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\model.py:474\u001b[0m, in \u001b[0;36mModel.process_text2video\u001b[1;34m(self, prompt, model_name, motion_field_strength_x, motion_field_strength_y, t0, t1, n_prompt, chunk_size, video_length, watermark, merging_ratio, seed, resolution, fps, use_cf_attn, use_motion_field, smooth_bg, smooth_bg_strength, path)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m     negative_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mguidance_stop_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmotion_field_strength_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmotion_field_strength_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmotion_field_strength_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmotion_field_strength_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_motion_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_motion_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msmooth_bg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmooth_bg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msmooth_bg_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmooth_bg_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmerging_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerging_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msplit_to_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mcreate_video(result, fps, path\u001b[38;5;241m=\u001b[39mpath, watermark\u001b[38;5;241m=\u001b[39mgradio_utils\u001b[38;5;241m.\u001b[39mlogo_name_to_path(watermark))\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\model.py:120\u001b[0m, in \u001b[0;36mModel.inference\u001b[1;34m(self, split_to_chunks, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_chunk(frame_ids\u001b[38;5;241m=\u001b[39mframe_ids,\n\u001b[0;32m    121\u001b[0m                                    prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    122\u001b[0m                                    negative_prompt\u001b[38;5;241m=\u001b[39mnegative_prompt,\n\u001b[0;32m    123\u001b[0m                                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m    124\u001b[0m frames_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk_ids)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_huggingspace \u001b[38;5;129;01mand\u001b[39;00m frames_counter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m:\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\model.py:79\u001b[0m, in \u001b[0;36mModel.inference_chunk\u001b[1;34m(self, frame_ids, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m ModelType\u001b[38;5;241m.\u001b[39mText2Video:\n\u001b[0;32m     78\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m frame_ids\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe(prompt\u001b[38;5;241m=\u001b[39mprompt[frame_ids]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     80\u001b[0m                  negative_prompt\u001b[38;5;241m=\u001b[39mnegative_prompt[frame_ids]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     81\u001b[0m                  latents\u001b[38;5;241m=\u001b[39mlatents,\n\u001b[0;32m     82\u001b[0m                  generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator,\n\u001b[0;32m     83\u001b[0m                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\text_to_video_pipeline.py:390\u001b[0m, in \u001b[0;36mTextToVideoPipeline.__call__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[0;32m    388\u001b[0m x_t1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_t1_1, x_t1_k], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m--> 390\u001b[0m ddim_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDDIM_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnull_embs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mguidance_stop_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_stop_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_warmup_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m x0 \u001b[38;5;241m=\u001b[39m ddim_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ddim_res\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\text_to_video_pipeline.py:152\u001b[0m, in \u001b[0;36mTextToVideoPipeline.DDIM_backward\u001b[1;34m(self, num_inference_steps, timesteps, skip_t, t0, t1, do_classifier_free_guidance, null_embs, text_embeddings, latents_local, latents_dtype, guidance_scale, guidance_stop_step, callback, callback_steps, extra_step_kwargs, num_warmup_steps)\u001b[0m\n\u001b[0;32m    149\u001b[0m         text_embeddings[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m null_embs[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    150\u001b[0m     te \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([repeat(text_embeddings[\u001b[38;5;241m0\u001b[39m, :, :], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc k -> f c k\u001b[39m\u001b[38;5;124m\"\u001b[39m, f\u001b[38;5;241m=\u001b[39mf),\n\u001b[0;32m    151\u001b[0m                    repeat(text_embeddings[\u001b[38;5;241m1\u001b[39m, :, :], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc k -> f c k\u001b[39m\u001b[38;5;124m\"\u001b[39m, f\u001b[38;5;241m=\u001b[39mf)])\n\u001b[1;32m--> 152\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mte\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mlatents_dtype)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\diffusers\\models\\unet_2d_condition.py:582\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m downsample_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(downsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m downsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m--> 582\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\diffusers\\models\\unet_2d_blocks.py:837\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    836\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m--> 837\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m    843\u001b[0m     output_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\diffusers\\models\\transformer_2d.py:265\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# 2. Blocks\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m--> 265\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\tomesd\\patch.py:103\u001b[0m, in \u001b[0;36mmake_diffusers_tome_block.<locals>.ToMeBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 1. Self-Attention\u001b[39;00m\n\u001b[0;32m    102\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 103\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m    104\u001b[0m     norm_hidden_states,\n\u001b[0;32m    105\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm_zero:\n\u001b[0;32m    110\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\diffusers\\models\\cross_attention.py:205\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# The `CrossAttention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m         hidden_states,\n\u001b[0;32m    208\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    209\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    211\u001b[0m     )\n",
      "File \u001b[1;32m~\\MyProjects\\Text2Videozero\\utils.py:218\u001b[0m, in \u001b[0;36mCrossFrameAttnProcessor.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    215\u001b[0m key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(key)\n\u001b[0;32m    216\u001b[0m value \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(value)\n\u001b[1;32m--> 218\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_probs, value)\n\u001b[0;32m    220\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mbatch_to_head_dim(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\v5\\lib\\site-packages\\diffusers\\models\\cross_attention.py:234\u001b[0m, in \u001b[0;36mCrossAttention.get_attention_scores\u001b[1;34m(self, query, key, attention_mask)\u001b[0m\n\u001b[0;32m    231\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     baddbmm_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 8.00 GiB total capacity; 3.12 GiB already allocated; 1.71 GiB free; 4.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "prompt = \"A horse galloping on a street\"\n",
    "params = {\"t0\": 44, \"t1\": 47 , \"motion_field_strength_x\" : 12, \"motion_field_strength_y\" : 12, \"video_length\": 8}\n",
    "\n",
    "out_path, fps = f\"./text2video_{prompt.replace(' ','_')}.mp4\", 4\n",
    "model.process_text2video(prompt, fps = fps, path = out_path, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c28aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb838db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae5b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
